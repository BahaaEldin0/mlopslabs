{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGExifOvrMXe",
        "outputId": "2a6b7abb-b951-4f82-a602-fb606a859c32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.13.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.8.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.29)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.3.3)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.11.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1V33RyparJOH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import optuna\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import reuters\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load the Reuters dataset\n",
        "max_len = 300  # Adjust based on the dataset analysis\n",
        "(X_train, y_train), (X_test, y_test) = reuters.load_data(path=\"reuters.npz\")\n",
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)\n",
        "\n",
        "# Convert the data to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Create a DataLoader\n",
        "batch_size = 32\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, n_hidden, n_units, dropout_rate):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        layers = [nn.Linear(input_dim, n_units), nn.ReLU(), nn.Dropout(dropout_rate)]\n",
        "\n",
        "        for _ in range(n_hidden):\n",
        "            layers += [nn.Linear(n_units, n_units), nn.ReLU(), nn.Dropout(dropout_rate)]\n",
        "\n",
        "        layers += [nn.Linear(n_units, output_dim)]\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # Flatten the input\n",
        "        logits = self.network(x)\n",
        "        return logits\n",
        "\n",
        "def create_model(trial, input_dim, output_dim):\n",
        "    n_hidden = trial.suggest_int('n_hidden', 3, 7)  # Increased range for hidden layers\n",
        "    n_units = trial.suggest_int('n_units', 64, 256)  # Increased range for units per layer\n",
        "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)  # Suggesting dropout rate\n",
        "    model = NeuralNetwork(input_dim, output_dim, n_hidden, n_units, dropout_rate)\n",
        "    return model\n",
        "\n",
        "\n",
        "def objective(trial):\n",
        "    model = create_model(trial, X_train.shape[1], len(np.unique(y_train)))\n",
        "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(10):\n",
        "        model.train()\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(X_batch)\n",
        "            loss = criterion(output, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            output = model(X_batch)\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            total += y_batch.size(0)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCuQrqS7wKTv",
        "outputId": "23bd9363-4349-4372-8aa2-4cffb1ccafbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-04-29 23:28:19,913] A new study created in memory with name: no-name-f2ac02da-005b-4db4-b970-8af254585a29\n",
            "<ipython-input-9-6ebd1db886f0>:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
            "[I 2024-04-29 23:28:46,542] Trial 0 finished with value: 0.41585040071237755 and parameters: {'n_hidden': 4, 'n_units': 118, 'dropout_rate': 0.1635270194911682, 'learning_rate': 0.00010649115779302927}. Best is trial 0 with value: 0.41585040071237755.\n",
            "[I 2024-04-29 23:29:03,082] Trial 1 finished with value: 0.37310774710596617 and parameters: {'n_hidden': 6, 'n_units': 231, 'dropout_rate': 0.1662249356461894, 'learning_rate': 0.00014561113817368877}. Best is trial 0 with value: 0.41585040071237755.\n",
            "[I 2024-04-29 23:29:18,402] Trial 2 finished with value: 0.37043633125556547 and parameters: {'n_hidden': 5, 'n_units': 246, 'dropout_rate': 0.460153946354373, 'learning_rate': 6.805959164180072e-05}. Best is trial 0 with value: 0.41585040071237755.\n",
            "[I 2024-04-29 23:29:42,393] Trial 4 finished with value: 0.3664292074799644 and parameters: {'n_hidden': 3, 'n_units': 252, 'dropout_rate': 0.2605012745244968, 'learning_rate': 0.0006029758418125183}. Best is trial 0 with value: 0.41585040071237755.\n",
            "[I 2024-04-29 23:29:49,908] Trial 3 finished with value: 0.3619768477292965 and parameters: {'n_hidden': 5, 'n_units': 200, 'dropout_rate': 0.14830765184688383, 'learning_rate': 0.06665328199217095}. Best is trial 0 with value: 0.41585040071237755.\n",
            "[I 2024-04-29 23:30:16,852] Trial 6 finished with value: 0.3619768477292965 and parameters: {'n_hidden': 3, 'n_units': 172, 'dropout_rate': 0.4126906036115183, 'learning_rate': 0.015623571513288236}. Best is trial 0 with value: 0.41585040071237755.\n",
            "[I 2024-04-29 23:30:24,874] Trial 5 finished with value: 0.3619768477292965 and parameters: {'n_hidden': 5, 'n_units': 193, 'dropout_rate': 0.2250042282899922, 'learning_rate': 0.09230362561161833}. Best is trial 0 with value: 0.41585040071237755.\n",
            "[I 2024-04-29 23:30:51,609] Trial 8 finished with value: 0.3624220837043633 and parameters: {'n_hidden': 5, 'n_units': 182, 'dropout_rate': 0.38732771385718945, 'learning_rate': 0.00012371637630616112}. Best is trial 0 with value: 0.41585040071237755.\n",
            "[I 2024-04-29 23:30:54,122] Trial 7 finished with value: 0.3619768477292965 and parameters: {'n_hidden': 5, 'n_units': 149, 'dropout_rate': 0.4252037338671053, 'learning_rate': 0.02412537442481257}. Best is trial 0 with value: 0.41585040071237755.\n",
            "[I 2024-04-29 23:31:06,180] Trial 9 finished with value: 0.39759572573463936 and parameters: {'n_hidden': 6, 'n_units': 162, 'dropout_rate': 0.17529135404926596, 'learning_rate': 8.937092965225526e-05}. Best is trial 0 with value: 0.41585040071237755.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n_hidden': 4, 'n_units': 118, 'dropout_rate': 0.1635270194911682, 'learning_rate': 0.00010649115779302927}\n"
          ]
        }
      ],
      "source": [
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=10, n_jobs=-1)  # Reduced the number of trials for brevity\n",
        "\n",
        "print(study.best_params)\n",
        "\n",
        "# Create a model with the best hyperparameters found\n",
        "best_model = create_model(study.best_trial, X_train.shape[1], len(np.unique(y_train)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crAw60hSwK0W",
        "outputId": "4a92980c-ba44-44df-a210-0b8bc80e1d8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the best model: 0.012911843276936777\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the best model on test data\n",
        "best_model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        output = best_model(X_batch)\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        total += y_batch.size(0)\n",
        "        correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Accuracy of the best model: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mP_AWTsrJDP7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}